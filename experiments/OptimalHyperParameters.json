{
    "graphsage + FedAvg on sider": 
    { "learning rate": 0.0015, 
        "dropout rate": 0.6,
        "node embedding dimension": 16, 
        "hidden layer dimension": 64,
        "readout embedding dimension": 64,
        "graph embedding dimension": 64,
        "attention heads": null,
        "alpha": null,
        "PARTITION_ALPHA": 0.2 
    },


    "graphsage + FedAvg on bbbp": 
    { "learning rate": 0.0015, 
        "dropout rate": 0.6,
        "node embedding dimension": 64, 
        "hidden layer dimension": 64,
        "readout embedding dimension": 64,
        "graph embedding dimension": 64,
        "attention heads": null,
        "alpha": null,
        "PARTITION_ALPHA": 0.2     
    },
    
    
    "graphsage + FedAvg on bace": 
    { "learning rate": 0.0015, 
        "dropout rate": 0.5,
        "node embedding dimension": 16, 
        "hidden layer dimension": 64,
        "readout embedding dimension": 64,
        "graph embedding dimension": 64,
        "attention heads": null,
        "alpha": null,
        "PARTITION_ALPHA": 0.2     
    },


    "graphsage + FedAvg on Tox21": 
    { "learning rate": 0.0015, 
        "dropout rate": 0.6,
        "node embedding dimension": 64, 
        "hidden layer dimension": 64,
        "readout embedding dimension": 64,
        "graph embedding dimension": 64,
        "attention heads": null,
        "alpha": null,
        "PARTITION_ALPHA": 0.2     
    },

    
    "graphsage + FedAvg on clintox": 
    { "learning rate": 0.0015, 
        "dropout rate": 0.6,
        "node embedding dimension": 64, 
        "hidden layer dimension": 64,
        "readout embedding dimension": 64,
        "graph embedding dimension": 64,
        "attention heads": null,
        "alpha": null,
        "PARTITION_ALPHA": 0.1     
    },

    "graphsage + FedAMP on sider": 
    { "learning rate": 0.0015, 
        "dropout rate": 0.6,
        "node embedding dimension": 16, 
        "hidden layer dimension": 64,
        "readout embedding dimension": 64,
        "graph embedding dimension": 64,
        "attention heads": null,
        "alpha": null,
        "PARTITION_ALPHA": 0.2,
        "alphaK": 1,
        "lamda": 0.1,
        "sigma": 1
    }
}